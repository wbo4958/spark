/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

pipeline {
    agent any

    options {
        ansiColor('xterm')
        timestamps()
        timeout(time: 6, unit: 'HOURS')
        gitLabConnection('GitLab Master')
        buildDiscarder(logRotator(numToKeepStr: '10'))
    }

    triggers {
        cron('H 6 * * *')
    }

    parameters {
        string(name: 'REF',
               defaultValue: '',
               description: 'Commit to build')

        string(name: 'COMMIT',
               defaultValue: '',
               description: 'Commit to build')
    }

    environment {
        URM_CREDS=credentials("svcngcc_artifactory")
    }

    stages {
                stage('hadoop-2.7 jars') {
                    agent {
                        dockerfile {
                            label 'docker'
                            args '-v ${HOME}/.m2:${HOME}/.m2:rw -v ${HOME}/.sparkStaging:${HOME}/.sparkStaging:rw -v ${HOME}/.ivy2:${HOME}/.ivy2:rw -v ${HOME}/.zinc:${HOME}/.zinc:rw -v /etc/passwd:/etc/passwd:ro'
                        }
                    }
                    steps {
                        checkout_commit()
                        updateGitlabCommitStatus(name: 'pipeline', state: 'running')
                        updateGitlabCommitStatus(name: 'make-distribution', state: 'running')
                        retry(3) {
                            sh './build/mvn -B -c -DskipTests --settings=jenkins/settings.xml -Phadoop-2.7 -Pyarn -Pkubernetes deploy'
                        }
                    }
                }

                stage('hadoop-3.2 tgz') {
                    agent {
                        dockerfile {
                            label 'docker'
                            args '-v ${HOME}/.m2:${HOME}/.m2:rw -v ${HOME}/.sparkStaging:${HOME}/.sparkStaging:rw -v ${HOME}/.ivy2:${HOME}/.ivy2:rw -v ${HOME}/.zinc:${HOME}/.zinc:rw -v /etc/passwd:/etc/passwd:ro'
                        }
                    }
                    steps {
                        checkout_commit()
                        updateGitlabCommitStatus(name: 'pipeline', state: 'running')
                        updateGitlabCommitStatus(name: 'make-distribution', state: 'running')
                        sh './dev/make-distribution.sh --tgz --name hadoop3.2 -Phadoop-3.2 -Pyarn -Pkubernetes -Phive'
                        script {
                            def SPARK_VER=sh(returnStdout: true,
                                script: "mvn help:evaluate -Dexpression=project.version -q -N -DforceStdout")
                            retry(3) {
                                sh "mvn -B -c deploy:deploy-file -DgroupId=org.apache -DartifactId=spark \
                                    -Dversion=$SPARK_VER -Dclassifier=bin-hadoop3.2 -Dpackaging=tgz \
                                    -Durl=https://urm.nvidia.com:443/artifactory/sw-spark-maven-local \
                                    -Dfile=spark-$SPARK_VER-bin-hadoop3.2.tgz \
                                    -s jenkins/settings.xml -DrepositoryId=snapshots"
                            }
                        }
                    }
                }
    }//end of stages

    post {
        success {
            slack("#rapidsai-spark-cicd", "Success", color: "#33CC33")
            build(job: 'spark/plugin_nightly', propagate: false, parameters: [string(name: 'REF', value: 'branch-0.1')])
        }
        failure {
            slack("#rapidsai-spark-cicd", "Failed", color: "#FF0000")
        }
        // failure condition doesn't trigger on unstable status
        unstable {
            slack("#rapidsai-spark-cicd", "Unstable", color: "#FFFF00")
        }
    }
}

def checkout_commit() {
if (!params.COMMIT?.trim()) {
        error "Must Specifiy parameter 'COMMIT'"
    }
    checkout([$class: 'GitSCM', branches: [[name: "${params.COMMIT}"]],
              userRemoteConfigs: [[credentialsId: 'svcngcc_pubpriv', url: "${GIT_URL}"]]])
}

void slack(Map params = [:], String channel, String message) {
    Map defaultParams = [
            color: "#000000",
            baseUrl: "https://nvidia.slack.com/services/hooks/jenkins-ci/",
            tokenCredentialId: "slack_token"
    ]

    params["channel"] = channel
    params["message"] = "${BUILD_URL}\n" + message

    slackSend(defaultParams << params)
}
